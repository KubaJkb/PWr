{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Regresja Liniowa i Logistyczna w czystym NumPy",
   "id": "22beda5e6648d6dd"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Wczytanie danych\n",
    "data = pd.read_csv(\"../data/ObesityDataSet.csv\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. CZĘŚĆ: Regresja liniowa (zamknięta forma)\n",
   "id": "f3942790e0b675fa"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Przewidujemy wagę (Weight) na podstawie innych cech numerycznych\n",
    "\n",
    "numerical_features = data.select_dtypes(include=['int64', 'float64']).columns.drop('Weight')\n",
    "# numerical_features = [\"Age\", \"Height\", \"FCVC\", \"NCP\", \"CH2O\", \"FAF\", \"TUE\"]\n",
    "X = data[numerical_features].values\n",
    "y = data[\"Weight\"].values.reshape(-1, 1)\n",
    "\n",
    "# Dodanie biasu (kolumna jedynek)\n",
    "X_b = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "\n",
    "# Zamknięta forma: theta = (X^T X)^(-1) X^T y\n",
    "theta_closed = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\n",
    "\n",
    "# Predykcja\n",
    "y_pred_closed = X_b @ theta_closed\n",
    "\n",
    "# Mean Squared Error\n",
    "mse_closed = np.mean((y - y_pred_closed) ** 2)\n",
    "print(\"Zamknięta forma - MSE:\", mse_closed)"
   ],
   "id": "cb22df7dc9f6b7c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2. CZĘŚĆ: Regresja Liniowa – Gradient Descent",
   "id": "fc039ff2f7e464c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Funkcja kosztu – Mean Squared Error\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# Funkcja obliczająca gradient MSE względem parametrów theta\n",
    "def mse_gradient(X, y, theta):\n",
    "    return (2 / X.shape[0]) * X.T @ (X @ theta - y)\n",
    "\n",
    "# Normalizacja danych\n",
    "X_scaled = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "X_b = np.hstack([np.ones((X_scaled.shape[0], 1)), X_scaled])\n",
    "\n",
    "# Inicjalizacja\n",
    "theta_gd = np.random.randn(X_b.shape[1], 1)\n",
    "learning_rate = 0.01\n",
    "n_epochs = 1000\n",
    "\n",
    "# Gradient descent\n",
    "\n",
    "# for epoch in range(n_epochs):\n",
    "#     gradients = mse_gradient(X_b, y, theta_gd)\n",
    "#     theta_gd -= learning_rate * gradients\n",
    "\n",
    "batch_size = 32\n",
    "n_samples = X_b.shape[0]\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Losowe przetasowanie danych\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    X_b_shuffled = X_b[indices]\n",
    "    y_shuffled = y[indices]\n",
    "\n",
    "    for i in range(0, n_samples, batch_size):\n",
    "        X_batch = X_b_shuffled[i:i+batch_size]\n",
    "        y_batch = y_shuffled[i:i+batch_size]\n",
    "\n",
    "        gradients = mse_gradient(X_batch, y_batch, theta_gd)\n",
    "        theta_gd -= learning_rate * gradients\n",
    "\n",
    "# Predykcja i MSE\n",
    "y_pred_gd = X_b @ theta_gd\n",
    "mse_gd = mse_loss(y, y_pred_gd)\n",
    "print(\"Gradient Descent - MSE:\", mse_gd)"
   ],
   "id": "c35bc9b973b59385",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. CZĘŚĆ: Regresja Logistyczna – Gradient Descent",
   "id": "6d626884ef9182a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Binarna klasyfikacja: czy ktoś jest \"Overweight\" lub \"Obesity\" vs reszta\n",
    "data[\"target_binary\"] = data[\"NObeyesdad\"].apply(lambda x: 1 if (\"Overweight\" in x or \"Obesity\" in x) else 0)\n",
    "\n",
    "# Proste cechy: Age i Weight\n",
    "numerical_cols = data.select_dtypes(include=['int64', 'float64']).drop(columns=[\"target_binary\"]).columns\n",
    "X_cls = data[numerical_cols].values\n",
    "y_cls = data[\"target_binary\"].values.reshape(-1, 1)\n",
    "\n",
    "# Normalizacja\n",
    "scaler = StandardScaler()\n",
    "X_cls_scaled = scaler.fit_transform(X_cls)\n",
    "X_cls_b = np.hstack([np.ones((X_cls_scaled.shape[0], 1)), X_cls_scaled])\n",
    "\n",
    "# Sigmoid\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Cross-Entropy Loss\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    eps = 1e-10\n",
    "    return -np.mean(y_true * np.log(y_pred + eps) + (1 - y_true) * np.log(1 - y_pred + eps))\n",
    "\n",
    "# Gradient descent for logistic regression\n",
    "batch_size = 32\n",
    "num_samples = X_cls_b.shape[0]\n",
    "theta_log = np.random.randn(X_cls_b.shape[1], 1)\n",
    "learning_rate = 0.05\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    indices = np.arange(num_samples)\n",
    "    np.random.shuffle(indices)  # tasowanie danych\n",
    "\n",
    "    for start_idx in range(0, num_samples, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, num_samples)\n",
    "        batch_idx = indices[start_idx:end_idx]\n",
    "\n",
    "        X_batch = X_cls_b[batch_idx]\n",
    "        y_batch = y_cls[batch_idx]\n",
    "\n",
    "        logits = X_batch @ theta_log\n",
    "        y_pred = sigmoid(logits)\n",
    "        gradients = X_batch.T @ (y_pred - y_batch) / y_batch.shape[0]\n",
    "        theta_log -= learning_rate * gradients\n",
    "\n",
    "# Ewaluacja\n",
    "predicted_classes = (sigmoid(X_cls_b @ theta_log) >= 0.5).astype(int)\n",
    "accuracy = np.mean(predicted_classes == y_cls)\n",
    "print(\"Logistyczna regresja (Gradient Descent) - Accuracy:\", accuracy)"
   ],
   "id": "11d3298040a48021",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. CZĘŚĆ: Porównanie z scikit-learn",
   "id": "651b8bac5bab33bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "\n",
    "# Regresja liniowa\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, y)\n",
    "mse_sklearn = np.mean((lr.predict(X) - y) ** 2)\n",
    "print(\"sklearn LinearRegression - MSE:\", mse_sklearn)\n",
    "\n",
    "# Regresja logistyczna\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_cls_scaled, data[\"target_binary\"])\n",
    "acc_sklearn = logreg.score(X_cls_scaled, data[\"target_binary\"])\n",
    "print(\"sklearn LogisticRegression - Accuracy:\", acc_sklearn)"
   ],
   "id": "5e3995024ff8a0a1",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
